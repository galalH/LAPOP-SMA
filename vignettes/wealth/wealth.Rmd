---
title: "Wealth index"
author: "Craig Jolley"
date: "September 29, 2015"
output: html_document
---

Here I'll be demonstrating how to use principal component analysis to build a composite wealth index from Americas Barometer indicators. We'll be looking at the following:

- `r3`: Refrigerator in home (0 = No, 1 = Yes)
- `r4`: Landline in home
- `r4a`: Cell phone in home
- `r5`: Number of vehicles owned (0 = None, 1 = one, 2 = two, 3 = three or more)
- `r6`: Washing machine in home
- `r7`: Microwave oven in home
- `r8`: Owns motorcycle
- `r12`: Drinking water in home
- `r14`: Indoor bathroom in home
- `r15`: Computer in home
- `r18`: Internet access in home
- `r1`: Television in home
- `r16`: Flat-panel TV in home
- `r26`: Home connected to sewage system
- `q10new`: Monthly household income (scale of 0-16)
- `q10g`: Monthly personal income (scale of 0-16)

To start off, we'll load the necessary packages:

```{r, message=FALSE}
library(ggplot2)
library(mice)
library(plyr) # nice for re-formatting data
library(GGally) # for plot matrices
```

We'll load all three 2014 datasets, and construct a data frame containing the columns we want.

```{r}
setwd("C:/Users/Craig/Desktop/SMA/VSFS/LAPOP-SMA/indices")
lapop.2014.GTM <- read.csv('../../2014-GTM.csv')
lapop.2014.SLV <- read.csv('../../2014-SLV.csv')
lapop.2014.HND <- read.csv('../../2014-HND.csv')
v <- c('r3','r4','r4a','r5','r6','r7','r8','r12','r14','r15','r18','r1','r16',
       'r26','q10new','q10g')
my_data <- rbind(lapop.2014.GTM[,v],lapop.2014.SLV[,v],lapop.2014.HND[,v])
my_data <- my_data[,order(names(my_data))] # alphabetize columns
```

We need to do something with missing values. It will be easier to work with them if we change them to NAs:

```{r}
is.na(my_data[my_data>16]) <- TRUE
nrow(na.omit(my_data))
```

If we just ignore any rows with missing values, we'll end up with only 647 rows left -- 41% of what we started with. How many values are we missing, anyway?

```{r}
sum(is.na(my_data)) / (nrow(my_data)*ncol(my_data))
```

So we're only missing about 4% of values -- it hurts my heart to throw away 59% of the data because of that. It also might introduce biases (for example if poor or rich people are more likely to not answer questions). For now, though, I'll endure the pain and see what PCA looks like with just the complete data:

```{r}
my_complete <- na.omit(my_data)
pr_complete <- prcomp(my_complete,center=TRUE,scale=FALSE)
plot(pr_complete)
summary(pr_complete)
```

83% of the variance is in the first component, which is pretty good. Note that people typically recommend using `scale=TRUE`; the reason I didn't is because one of our variables (`r1`, TV in home) has a variance of zero in this dataset -- everyone counted said "yes". This gives us a divide-by-zero error when we scale, which is no good. 

We can look at a scatterplot of the first two principal components:

```{r,warning=FALSE}
qplot(pr_complete$x[,1],pr_complete$x[,2],xlim=c(-10,10),ylim=c(-10,10)) + 
  theme_classic()
```

These are the two principal components along which the variation is the most spread-out. The lattice-like pattern comes from the fact that we're seeing a projection of variables that can only take on certain discrete values, as opposed to continuous ones. In the ideal case, most of the variance would be in the first component, but this isn't too bad. If you want to see how some of the other dimenisions look, try changing the values 1 and 2 to others, up to 17 (which should contain the least variance).

Now let's try imputing those missing values.

There are a few different multiple imputation schemes available in R, but it's hard to find a good explanation of them. [This one](http://pj.freefaculty.org/guides/Rcourse/multipleImputation/multipleImputation-1-lecture.pdf) is OK, for being a really long powerpoint. I'll try doing this using the `mice` package (Multivariate Imputation by Chained Equations) and see how it does. More about MICE [here](http://www.stefvanbuuren.nl/mi/MICE.html).

```{r}
my_imp <- mice(my_data,printFlag = F,seed=12345) 
nmis <- my_imp$nmis
nm <- data.frame(nmis=nmis/nrow(my_data),var=names(nmis))
ggplot(nm,aes(x=var,y=nmis)) +
  geom_bar(stat = 'identity', fill = '#90AFC5') +
  ylab('Fraction of imputed values') +
  coord_flip() +
  theme_classic() +
  theme(legend.position='none',
        axis.ticks=element_blank(),
        axis.line=element_blank(),
        axis.title.y=element_blank(),
        text=element_text(size=20))
```

There's a lot going on in the output here. MICE generated 5 different imputed data sets, so that we can run the same analyses on each and see how they compare. Some columns have a lot more missing cells than others -- everybody knows whether they have a washing machine (`r6`), but flat-panel TVs (`r16`) caused some confusion, and a lot of people didn't disclose their personal income (`q10g`). The PredictorMatrix at the bottom shows which variables were used to predict others. Except for `r6` and `r15`, which didn't need predicting at all, it looks like everything was used to predict everything else. 

```{r}
v2 <- c('q10g','pais')
q10g <- rbind(lapop.2014.GTM[,v2],lapop.2014.SLV[,v2],lapop.2014.HND[,v2])
countries <- c('','Guatemala','El Salvador','Honduras')
q10g <- q10g[q10g$q10g > 800000,]
q10g$pais_text <- countries[q10g$pais]
table(q10g$pais_text)
```

Fortunately, non-responses to `q10g` came from the three countries in roughly equal fractions.

Here's an example from one row that had some missing values (`r1`, `r16`, and `q10g`):

```{r}
my_data[1,]
t(sapply(1:5,function(i) complete(my_imp,i)[1,]))
```

This family doesn't have much stuff and their income is low, but 3 of the 5 predictions gave them a flat-panel TV. These might not be reliable predictions. The personal income (`q10g`) varies between imputed datasets, but tends to be at the low end and is probably more trustworthy.

Now let's do the PCA calculation. We'll generate a structure called `pr` that
contains the results of 5 different PCA calculations for the 5 imputed datasets.
```{r}
pr <- lapply(1:5,function(x) prcomp(complete(my_imp,x),scale=FALSE,center=TRUE))
```

We can visualize the fraction of variance contained in each of the 17 principal components to see to what extent variance is concentrated in the first few. (Do this for just the first imputed model.)

```{r}
plot(pr[[1]])
summary(pr[[1]])
```

About 85% of the total variance is concentrated in the first component, roughly the same as before imputation -- this means our imputation didn't mess things up too much (not yet, at least). What if we make a scatter plot of the first two components (for just one of the imputed datasets)?

```{r}
pc1 <- pr[[1]]$x[,1]
pc2 <- pr[[1]]$x[,2]
qplot(pc1,pc2) + theme_classic()
```

We want to be sure that all five of our imputed datasets are measuring the same thing. We can do this by plotting all of the pairwise scatterplots; R makes this easy for us, if we put them all in one data frame first.

```{r, fig.width=10, fig.height=10}
all_pc1 <- data.frame(llply(1:5, function(i) pr[[i]]$x[,1]))
names(all_pc1) <- c('imp1','imp2','imp3','imp4','imp5')
ggpairs(all_pc1) + theme_classic()
```

What is this showing? `imp1` through `imp5` are our five imputed datasets, and we're looking at the first principal component from each. On the diagonal of this matrix, we see distribution plots for each of them -- they all look really similar. Above the diagonal, we get pairwise correlations; for example `imp1` and `imp2` have a correlation of 0.912. All of these are fairly high. Below the diagonal, we see scatter plots for each pair of imputations. There are some disagreements, but in general they match up quite well. Averaging all five should help smooth out some of those cases where they disagree.

```{r,fig.height=5,fig.width=10}
all_pc1$avg <- rowMeans(all_pc1)
q25 <- quantile(all_pc1$avg)[2]
q75 <- quantile(all_pc1$avg)[4]
qchange <- data.frame(low=colMeans(my_data[all_pc1$avg <= q25,],na.rm=TRUE),
                      hi=colMeans(my_data[all_pc1$avg >= q75,],na.rm=TRUE))
qchange[c('q10new','q10g'),'low'] <- 0.1*qchange[c('q10new','q10g'),'low']
qchange[c('q10new','q10g'),'hi'] <- 0.1*qchange[c('q10new','q10g'),'hi']
qchange$slope <- qchange$hi - qchange$low
qchange$label <- rownames(qchange)
qchange$even <- rank(qchange$hi)/nrow(qchange)*max(c(qchange$low,qchange$hi))

ggplot(data=qchange) +
  geom_segment(aes(x=0,y=qchange$low,xend=1,yend=qchange$hi,
                   color=qchange$slope),size=1) +
  scale_color_gradient(low='#FF420E',high='#89DA59') +
  geom_text(aes(x=1,y=qchange$even,label=qchange$label,
                hjust=0,color=qchange$slope),size=5) +
  annotate("text",x=0,y=0,label='Average in lowest quartile',hjust=0) +
  annotate("text",x=1,y=0,label='Average in highest quartile',hjust=1) +
  scale_x_continuous(limits=c(0,1.5)) +
  theme_classic() +
  theme(legend.position='none',
        axis.ticks=element_blank(),
        axis.line=element_blank(),
        axis.text.x=element_blank(),
        axis.title=element_blank(),
        text=element_text(size=20))
```

The `q10` variables increase dramatically from the lowest to the highest quartile, which makes sense as these are direct measures of income. The other variables all show more modest increases -- rich people own more things, but a household's assets are a function of more than just their income.

We can see more details about how the top and bottom quantiles differ. For example, nearly everyone in the top 25% has a refrigerator (`r3`), a cell phone (`r4a`), indoor drinking water (`r12`), and a television (`r1`), but motorcycles (`r8`) are pretty rare. In the bottom 25%, more people have cell phones (`r4a`) than indoor bathrooms (`r14`).

As another sanity check, let's look at the distribution of values from our composite index. First, we'll figure out what the hypothetical maximum and minimum values of the wealth index are, then we'll plot those together with the actual density distribution.

We'll adjust the score to have a mean of 0 and standard deviation of 1. The new value will be in a column called `norm`.

```{r}
all_pc1$norm <- scale(all_pc1$avg)
predict_data <- data.frame(r3=c(0,1),r4=c(0,1),r4a=c(0,1),r5=c(0,3),r6=c(0,1),
                           r7=c(0,1),r8=c(0,1),r12=c(0,1),r14=c(0,1),
                           r15=c(0,1),r18=c(0,1),r1=c(0,1),r16=c(0,1),
                           r26=c(0,1),q10new=c(0,16),q10g=c(0,16))
minmax <- rowMeans(sapply(1:5, function(i) predict(pr[[i]],predict_data)[,1]))
minmax <- (minmax - mean(all_pc1$avg)) / sd(all_pc1$avg)
wealth <- data.frame(w=all_pc1$norm) 
ggplot(wealth,aes(x=w)) +
  geom_density(fill='bisque2',linetype='blank') +
  geom_segment(x=minmax[1],xend=minmax[1],y=0,yend=0.5,size=2,color='darkred') +
  geom_segment(x=mean(wealth$w),xend=mean(wealth$w),y=0,yend=0.5,size=2,color='bisque4') +
  geom_segment(x=minmax[2],xend=minmax[2],y=0,yend=0.5,size=2,color='darkolivegreen4') +
  annotate('text',label=paste("Lowest:",round(minmax[1],2)),  
           size=7,x=minmax[1]+0.05,y=0.4,hjust=0,vjust=0,color='darkred') +
  annotate('text',label="Mean: 0",  
           size=7,x=0.05,y=0.4,hjust=0,vjust=0,color='bisque4') +
  annotate('text',label=paste("Highest:",round(minmax[2],2)),  
           size=7,x=minmax[2]-0.05,y=0.4,hjust=1,vjust=0,color='darkolivegreen4') +
  theme_classic() +
  theme(text=element_text(size=20),
        axis.title.y=element_blank(),
        axis.title.x=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank(),
        axis.line.y=element_blank()) 
```

Our data uses the full range of possible wealth values, and tends to be somewhat bunched up at the low end.

To understand our index better, we can ask how different types of responses contribute to it. In other words, how much does it add to someone's wealth index if they have a TV, or a motorcycle, or if their income increases by one level on the 16-point scale?

```{r,warning=FALSE}
predict_data2 <- data.frame(diag(16))
names(predict_data2) <- names(predict_data)
scores <- rowMeans(sapply(1:5, function(i) predict(pr[[i]],predict_data2)[,1]))
scores <- (scores - mean(all_pc1$avg)) / sd(all_pc1$avg)
diff <- data.frame(d=(scores - minmax[1]),n=names(predict_data))
diff_r <- diff[!diff$n %in% c('q10new','q10g'),]
diff_q <- diff[diff$n %in% c('q10new','q10g'),]
ggplot(diff_r,aes(x=n,y=d)) +
  geom_bar(stat='identity',fill='skyblue') +
  coord_flip() +
  theme_classic() +
  theme(text=element_text(size=20),
        axis.title.y=element_blank(),
        axis.title.x=element_blank())
```
```{r,fig.width=7,fig.height=2}
ggplot(diff_q,aes(x=n,y=d)) +
  geom_bar(stat='identity',fill='skyblue') +
  coord_flip() +
  theme_classic() +
  theme(text=element_text(size=20),
        axis.title.y=element_blank(),
        axis.title.x=element_blank())
```

Clearly, the indicators that measure income directly have a much larger influence on our wealth index than the others. Each 1-point increase in household income (`q10new`) increases the wealth index by 0.13, while the increases based on possessions average to about 0.004. Having a microwave oven (`r7`) increases a person's wealth index more than having a cell phone (`r4a`). Even poor people have cell phones, while microwaves tend to show a large difference between rich and poor households. 

How do we know whether this is really a good composite index? We've seen a few good signs already -- it captures most of the variance in the underlying indicators, it's robust to random variations in multiple imputation, and it's fairly nicely-distributed. The other thing to look for is how well it does at predicting the values of the individual variables -- a good wealth index should do a reasonable job of predicting how much income someone has and which things they own.

First, let's look at income (`q10new`):

```{r}
q10new <- data.frame(q=my_data$q10new,w=wealth$w)
q10new <- na.omit(q10new)
my_lm <- lm(data=q10new, q ~ w)
summary(my_lm)
ggplot(q10new,aes(x=w,y=q)) +
  geom_jitter(color='tomato3',size=5,alpha=0.2) +
  theme_classic() +
  geom_smooth(method='lm',size=2,color='royalblue') +
  annotate('text',label=paste("R-sq:",round(summary(my_lm)$r.squared,2)),  
           size=10,x=-0.5,y=14,hjust=0,vjust=0,color='royalblue') +
  ylab('q10new') +
  xlab('Wealth index') +
  theme(text=element_text(size=20)) 
```

Now, what about microwave ownership (`r7`)?

```{r}
r7 <- data.frame(r=my_data$r7,w=wealth$w)
r7 <- na.omit(r7)
my_glm <- glm(data=r7, r ~ w, family=binomial(logit))
summary(my_glm)
ggplot(r7,aes(x=w,y=r)) +
  geom_jitter(color='tomato3',size=5,alpha=0.2,position=position_jitter(0.1,0.1)) +
  theme_classic() +
  stat_smooth(method="glm", family="binomial",size=2,color='royalblue',se=FALSE) +
  ylab('r7') +
  xlab('Wealth index') +
  theme(text=element_text(size=20),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank()) 
```

This fit isn't quite as pretty as `q10new` was, but the summary from the logistic regression calculation tells us that our wealth index has a highly-significant impact on microwave oven ownership. We can also see on the plot that the non-owners (`r7` = 0) are bunched up at the low-wealth end of the spectrum, as we'd expect. It should be easy to modify these plots for some of the other variables; you can see which ones are well-predicted by our wealth index and which ones might not be as much.

We can test this more rigorously using 2-sample t-tests:

```{r}
bin_cor <- function(var) {
  tt <- t.test(all_pc1$norm[var==min(var,na.rm=TRUE)],
               all_pc1$norm[var==max(var,na.rm=TRUE)])
  tt$p.value
}

bin_vars <- c('r3','r4','r4a','r6','r7','r8','r12','r14','r15','r18','r1',
              'r16','r26')
sapply(bin_vars,function(x) bin_cor(my_data[,x]))
```

All of our p-values on logistic regressions are highly significant, meaning that our wealth index has a strong influence on all of these wealth-related variables. This is what we would expect! Notice that the smallest p-values (i.e. the highest significance) are for variables like `r14` and `r18` where the influence on the wealth index was higher. This is also what we would expect; variables like `r8` are still significant but not as strong.

Finally, check these values against the comments in `make_indices.R` to be sure we're calculating the same index:

```{r}
quantile(all_pc1$norm)
```